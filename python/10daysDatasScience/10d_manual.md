# 10天100小时学数据科学

说实话，我不觉得学习在线课程对你这次的学习有什么用，上课也不过就能给你点成就感罢了。如果你有不错的数学和编程基础，那么直接上手写代码、跑数据、看结果，比上在线课程有意思得多。那可是一百个小时呀！如果你真能在接下来十天的一百个小时里尽自己最大的努力学习，就能学到世界级专家所需知识的百分之一。时间紧任务重，你，准备好了嘛？

库和算法的内部实现之类的将来有得是时间学，你现在还用不上。我下面列出的任务量巨大，需要你全身心投入，目标是让你能够广泛地接触到这个领域的一些主要工具。

#### 第一天&第二天
下载 StackExchange 的公开数据：下载链接在此（需梯子）

处理数据需要关系数据库管理系统，第一天大概要干这些：

安装 MySQL 配置好后把上面下载的数据导入数据库

阅读 SQL 基础知识。花点时间做几个小练习题来熟悉数据操作。比如说，写个能够抽取所有满足下面条件的提问的脚本：提问是关于 Python 和 SQL的、回答多于三个、最佳回答的作者在这两个主题下拥有大于十个被选中的回答。你有可能会发现脚本性能有问题。

阅读 SQL 索引知识，了解哈希和排序等。修改上面的脚本让它能立刻马上跑出结果。

写个能处理上面查询语句的 Python 类。这就需要学习 Python MySQL driver。你需要一个工具，能帮助你从数据库中抽取数据并且能把它们以比较方便的形式呈现出来。

虽不清楚题主底子怎么样，但我觉得上面的任务即使对菜鸟来说也是完全能够完成的。你只要有些基础的 Python 知识就足够了。

#### 第二天
可以用来了解用 pandas 来读取数据，以及用 numpy 来对数值型数据进行操作。这些库的文档看起来页数多很吓人，但不用都读。只要学会导入 CSV 文件、添加提取数据列、合并两个数据库这些操作就行了。

#### 第三天
虽然实际工作中通常都是在整个数据库上做查询，但学习如何在小量级的数据做操作并且得到有意义的结果也是相当重要的。比如说，可以试着从整个数据集中自己随机抽取一些数据，然后把它们的得分的分布和整个数据集上的得分分布做比较。

现在可以再进一步了。虽然手握整个 Stack Exchange 的数据库，但是因为我对 StackOverflow 里面的数据比较熟悉一点，下面只会用到 StackOverflow 的数据。我能想到的一个有意思的练习：根据时间看编程语言的流行程度。

**为什么这个练习有用呢？**

如何抽取与仅仅编程语言本身有关的提问而不抽取只与技术有关的提问呢？（比如我想要和 Python 语法有关的提问，而不想要那种问如何在 Django 中用 MangoDB 的提问）

提问的标签数量非常大，为了最后将结果可视化需要谨慎挑选输入数据

可以学习到至少一个可视化框架

可以生成很多美图

会做上面的例子，自然而然就可以探寻更多数据里有意思的属性了。学会问问题是一个很重要的能力。

#### 第四天和第五天

数据科学家曾被评为“21 世纪最性感的职业”。你知道还有什么很性感么？图论哦！

问题标签之间是如何联系在一起的？是否能仅仅用 Stack Overflow 上的回答就构建一个和技术有关的图？该选哪个标准来计算两个标签有多相似？图的可视化该如何做？试 Gephi 了吗？

都做好了之后，需要给上面生成的图添加描述。仅仅一张图本身能提供的价值有限，你需要一直盯着它看，直到理解它背后所代表的意义。

学习聚类算法 （至少要学 k-means 和 DBSCAN）和 K 近邻算法。要是愿意钻研的话，还可以试试各种图论算法，算图的各项指标。建议使用 networkx 这个库和 scikit-learn 库里的一些相关部分，这些库能大大简化任务的难度。

**做这个有什么用？**

可以接触到不同格式的数据，比如 CSV， GEPHI， 边的集合等。

K-means 是一个有用的算法，学了不吃亏，以后用得上

研究数据的时候，发掘有意义的聚类是最重要的任务之一

可以根据自己的情况在这两天之中分配任务。我会推荐头一天玩玩 networkx 和 Gephi。第二天我会做聚类分析，因为做聚类的时候会有一些有挑战的问题，比如你需要思考：到底该用怎样的向量来表示问题标签才能保留它们之间的距离呢？！

#### 第六天
到了第六天应该已经基本明白数据库部分了。然而文本还没有接触过，只会算字数不算（此处有冷冷的一语双关）。

今天应该用来简单学点文本分析。学学潜在语义索引就够了，所有需要的东西在 scikit-learn 库里都有，还需要用一些 SQL。一般的流程是：

选出想用的数据
	
用 scikit 来抽取文本特征 （建议用 scikit 里的 TF-IDF Vectorizer）
	
给文本加标签。你可以做一个简单的练习：根据回答的文本来预测它会得多少分。这里文本的得分就是它的标签，TF-IDF 可以作为特征向量。
	
最好是能用 numpy 格式来为每个假设准备一个数据集。比如：

一个用来预测回答的得分
	
对回答按主题分类 （可以选择二十种编程语言的一些回答作为样本）

一定要注意搜集到的数据集要清理干净，你要确切地知道里面都有些什么。说起来简单做起来难。

#### 第七、八、九天
前一天中已经得到了干净的数据集。假设一个用于分类一个用于预测（在第五天时已经学过它们的区别了）。在这几天（译者注：原文写的第五天应是笔误）该集中学习回归模型了。scikit 库里提供了很全面的工具。应该上手试试下面提到的方法里至少三种：

线性模型。线性模型的种类浩如烟海。首先要比较它们的性能，然后读读最好的线性模型的相关知识，了解不同模型的区别。提示：好好学学 ElasticNet 回归。如果你数学还行的话可以读读 Bishop 的《模式识别与机器学习》 一书。书里很好地讲解了 ElasticNet 回归模型好用的原因。如果没时间的话可以不看。

回归树。

KNN 回归。KNN 通常很好用，不要瞧不起这些简单的方法。

集成学习模型如随机森林和自适应增强学习。

学习的主要目标并不是立刻变成这些算法的专家，而应该先跑起来代码，好用了之后然后再问问题。

同样的方法也适用于分类问题。思考应该用什么指标才能衡量结果的好坏。假设要建立一个给新闻排序的智能信息平台，该如何评估它的好坏呢？

对所有的模型做交叉验证是必不可少的。阅读 k 折（k-fold）交叉验证有关内容，研究如何用 scikit 来做 k 折交叉验证，然后对所有你之前建立过的模型都做一遍交叉验证。

#### 第十天
既然你想做一个数据科学家，这次经历不能落下它最有意思的部分，那就是展示结果。

无论你想选什么样的形式来展示它们都无所谓（将来很难有这样自由选择展示形式的机会了）。无论是半学术论文形式、PPT 展示、博客文章，还是一个手机 app 都可以，任君选择。把你的故事分享给大家。写写你在数据集里发现了什么、都做了些什么假设，分析假设能否成立的原因、简单描述下用到的算法、用简洁明了的形式来展示交叉验证的结果等，并且一定要多多放些图表。

这部分无论做到什么程度都不算用力过猛。我保证，如果你真能做一个好的展示并且展示给自己的伯乐看，入门 offer 指日可待。

